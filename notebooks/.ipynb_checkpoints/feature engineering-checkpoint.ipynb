{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from plotnine import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier \n",
    "from sklearn.ensemble import BaggingClassifier \n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy import interpolate\n",
    "    \n",
    "import statsmodels.api as sm\n",
    "import itertools\n",
    "\n",
    "from scipy.stats import mode\n",
    "\n",
    "import random\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('classic')\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:\\\\Users\\\\i053131\\\\Desktop\\\\Epilepsie\\\\CFM\\\\data\\\\raw\\\\training_input.csv', sep=';')\n",
    "test = pd.read_csv('C:\\\\Users\\\\i053131\\\\Desktop\\\\Epilepsie\\\\CFM\\\\data\\\\raw\\\\testing_input.csv', sep=';') \n",
    "output = pd.read_csv('C:\\\\Users\\\\i053131\\\\Desktop\\\\Epilepsie\\\\CFM\\\\data\\\\raw\\\\challenge_output_data_training_file_volatility_prediction_in_financial_markets.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = output[\"TARGET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_ = train.iloc[:, 57:]\n",
    "returnTest = test.iloc[:, 57:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volatility = train.iloc[:,3:57]\n",
    "volatilityTest=test.iloc[:,3:57]\n",
    "volatility.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double passe sur la serie - reecrire\n",
    "def count_NA(s):\n",
    "    result = 0\n",
    "    if s.isna().any():\n",
    "        result = s.value_counts(dropna=False).loc[np.nan]\n",
    "        \n",
    "    return result\n",
    "\n",
    "def count_ones(s):\n",
    "    result = 0\n",
    "    if (1 in s.unique()):\n",
    "        result = s.value_counts().loc[1]\n",
    "    return result\n",
    "\n",
    "def count_zeros(s):\n",
    "    result = 0\n",
    "    if (0 in s.unique()):\n",
    "        result = s.value_counts().loc[0]\n",
    "    return result\n",
    "\n",
    "def count_minus(s):\n",
    "    result = 0\n",
    "    if (-1 in s.unique()):\n",
    "        result = s.value_counts().loc[-1]\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_sum = return_.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(return_sum)\n",
    "plt.title(\"return sum\")\n",
    "plt.xlabel(\"rows return sum \")\n",
    "plt.ylabel(\"#\")\n",
    "#plt.yscale('log', nonposy='clip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_abssum = np.abs(return_).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(return_abssum)\n",
    "plt.title(\"return sum of return absolute value\")\n",
    "#plt.xlabel(\"rows return sum \")\n",
    "plt.ylabel(\"#\")\n",
    "#plt.yscale('log', nonposy='clip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.iloc[:,1:57].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add to ids and volatility \n",
    "#  % of NA in volatility\n",
    "#  % of 1, 0, -1 in return \n",
    "# sum and sum of absolute values of return \n",
    "\n",
    "def build_features_1(df):\n",
    "    return_ = df.iloc[:, 57:]\n",
    "    volatility = df.iloc[:,3:57]\n",
    "    result= df.iloc[:,:57].copy()\n",
    "    #feature NA count\n",
    "    lna=[]\n",
    "    lones=[]\n",
    "    lzeros =[]\n",
    "    lminus =[]\n",
    "    for i in df.index:\n",
    "        v = count_NA(volatility.iloc[i, :])/volatility.shape[1]\n",
    "        lna.append(v)\n",
    "        lones.append(count_ones(return_.iloc[i, :])/return_.shape[1])\n",
    "        lzeros.append(count_zeros(return_.iloc[i, :])/return_.shape[1])\n",
    "        lminus.append(count_minus(return_.iloc[i, :])/return_.shape[1])\n",
    "        if (i % (round(volatility.shape[0]/10)) ==0):\n",
    "            print(i, \"tenth of first part completed\")\n",
    "    \n",
    "    result[\"NA\"]=pd.Series(lna)\n",
    "    result[\"return_ones\"] = pd.Series(lones)\n",
    "    result[\"return_zeros\"] = pd.Series(lzeros)\n",
    "    result[\"return_minusones\"] = pd.Series(lminus)\n",
    "    \n",
    "    # features return sum and sum of the absolute values \n",
    "    result[\"return_sum\"]=  return_.sum(axis=1)\n",
    "    result[\"return_abssum\"] = np.abs(return_).sum(axis=1)\n",
    "\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf = build_features_1(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(fdf[\"NA\"])\n",
    "plt.title(\"nan occurence in rows in train volatility (%)\")\n",
    "plt.xlabel(\"% of NaN\")\n",
    "plt.ylabel(\" log Frequency\")\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(fdf[\"return_ones\"])\n",
    "plt.title(\"1 occurences in return (%)\")\n",
    "plt.ylabel(\" log Frequency\")\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(fdf[\"return_zeros\"])\n",
    "plt.title(\"0 occurences in return (%)\")\n",
    "plt.ylabel(\" log Frequency\")\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(fdf[\"return_minusones\"])\n",
    "plt.title(\"-1 occurences in return (%)\")\n",
    "plt.ylabel(\"log Frequency\")\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.hist(fdf[\"return_sum\"])\n",
    "plt.title(\"return sum\")\n",
    "plt.xlabel(\"rows return sum \")\n",
    "plt.ylabel(\"#\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(fdf[\"return_abssum\"])\n",
    "plt.title(\"return sum of return absolute value\")\n",
    "#plt.xlabel(\"rows return sum \")\n",
    "plt.ylabel(\"#\")\n",
    "#plt.yscale('log', nonposy='clip')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trying  straighforward xgboost on volatility + new features (build_features_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training, valid  = train_test_split(pd.concat([fdf, output[\"TARGET\"]], axis=1), test_size=0.2, random_state=42, stratify=fdf.iloc[:,1:2])\n",
    "#df= pd.concat([train, output[\"TARGET\"]], axis=1)\n",
    "#df=fdf\n",
    "df= pd.concat([fdf, output[\"TARGET\"]], axis=1)\n",
    "valid = df[df.date>=1563]\n",
    "training = df[df.date<1563]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training.iloc[:,1:56]\n",
    "y = training.iloc[:,-1]\n",
    "\n",
    "X_valid = valid.iloc[:,1:56]\n",
    "y_true = valid.iloc[:,-1]\n",
    "logy = np.log(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAPE(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "def MAPE_residus(y_true, y_pred):\n",
    "    return np.abs((y_true - y_pred) / y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "Lm = []\n",
    "Lt= []\n",
    "Ld = []\n",
    "r = range(50, 210, 10)\n",
    "for i in r:    \n",
    "    xbr= xgb.XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=i,  subsample=1,  \n",
    "                       random_state=42, n_jobs=-1)\n",
    "    xbr.fit(X, logy)\n",
    "\n",
    "    logy_pred = xbr.predict(X_valid)\n",
    "    y_pred = np.exp(logy_pred )\n",
    "    m= MAPE(y_true, y_pred)\n",
    "    logyt = xbr.predict(X)\n",
    "    yt = np.exp(logyt)\n",
    "    t= MAPE(y, yt)                \n",
    "    print(\"mape valid:    \"+ str(i), m)\n",
    "    print(\"mape training: \"+ str(i), t)\n",
    "    print(\"delta\", m-t)\n",
    "    print(\"--------------\")              \n",
    "    \n",
    "    Lm.append(m)\n",
    "    Lt.append(t)\n",
    "    Ld.append(m-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(r, Lt, label = \"train\")\n",
    "plt.plot(r, Lm, label = \"valid\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importances = xbr.feature_importances_\n",
    "feature_importances = pd.DataFrame(importances, index = X.columns, \n",
    "                                   columns=['importance']).sort_values('importance', ascending=False)\n",
    "plt.bar(feature_importances.index, feature_importances[\"importance\"])\n",
    "plt.show()\n",
    "\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### investidating depth\n",
    "- more indices of overfitting more than anything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Lm = []\n",
    "Lt= []\n",
    "Ld = []\n",
    "r = range(3, 10)\n",
    "for i in r:    \n",
    "    xbr= xgb.XGBRegressor(max_depth=i, learning_rate=0.1, n_estimators=140,  subsample=1,  \n",
    "                       random_state=42, n_jobs=-1)\n",
    "    xbr.fit(X, logy)\n",
    "\n",
    "    logy_pred = xbr.predict(X_valid)\n",
    "    y_pred = np.exp(logy_pred )\n",
    "    m= MAPE(y_true, y_pred)\n",
    "    logyt = xbr.predict(X)\n",
    "    yt = np.exp(logyt)\n",
    "    t= MAPE(y, yt)                \n",
    "    print(\"mape valid:    \"+ str(i), m)\n",
    "    print(\"mape training: \"+ str(i), t)\n",
    "    print(\"delta\", m-t)\n",
    "    print(\"--------------\")              \n",
    "    \n",
    "    Lm.append(m)\n",
    "    Lt.append(t)\n",
    "    Ld.append(m-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(r, Lt, label = \"train\")\n",
    "plt.plot(r, Lm, label = \"valid\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with all but date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.concat([fdf, output[\"TARGET\"]], axis=1)\n",
    "valid = df[df.date>=1563]\n",
    "training = df[df.date<1563]\n",
    "X = training.iloc[:,1:-1]\n",
    "y = training.iloc[:,-1]\n",
    "\n",
    "X_valid = valid.iloc[:,1:-1]\n",
    "y_true = valid.iloc[:,-1]\n",
    "logy = np.log(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Lm = []\n",
    "Lt= []\n",
    "Ld = []\n",
    "r = range(80, 170, 30)\n",
    "for i in r:    \n",
    "    xbr= xgb.XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=i,  subsample=1,  \n",
    "                       random_state=42, n_jobs=-1)\n",
    "    xbr.fit(X, logy)\n",
    "\n",
    "    logy_pred = xbr.predict(X_valid)\n",
    "    y_pred = np.exp(logy_pred )\n",
    "    m= MAPE(y_true, y_pred)\n",
    "    logyt = xbr.predict(X)\n",
    "    yt = np.exp(logyt)\n",
    "    t= MAPE(y, yt)                \n",
    "    print(\"mape valid:    \"+ str(i), m)\n",
    "    print(\"mape training: \"+ str(i), t)\n",
    "    print(\"delta\", m-t)\n",
    "    print(\"--------------\")              \n",
    "    \n",
    "    Lm.append(m)\n",
    "    Lt.append(t)\n",
    "    Ld.append(m-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(r, Lt, label = \"train\")\n",
    "plt.plot(r, Lm, label = \"valid\")\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = xbr.feature_importances_\n",
    "feature_importances = pd.DataFrame(importances, index = X.columns, \n",
    "                                   columns=['importance']).sort_values('importance', ascending=False)\n",
    "plt.bar(feature_importances.index, feature_importances[\"importance\"])\n",
    "plt.show()\n",
    "\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= valid.copy()\n",
    "df[\"ypred\"]= y_pred\n",
    "df[\"residus\"]= MAPE_residus(df[\"TARGET\"], df[\"ypred\"])\n",
    "\n",
    "dftrain= training.copy()\n",
    "dftrain[\"yt\"]= yt\n",
    "dftrain[\"residus\"]= MAPE_residus(dftrain[\"TARGET\"], dftrain[\"yt\"])\n",
    "\n",
    "\n",
    "\n",
    "plt.plot( 'TARGET', 'residus', data=df, linestyle='', marker='o', markersize=2)\n",
    "plt.yscale('log', nonposy='clip')\n",
    "#ax.set_xlim(0,)\n",
    "plt.title(\"|ypred-y|/y vs y on valid ln Q - original split\")\n",
    "plt.show()\n",
    "\n",
    "ax = plt.subplot()\n",
    "ax.set_xscale(\"log\", nonposx='clip')\n",
    "ax.set_yscale(\"log\", nonposy='clip')\n",
    "plt.plot( 'TARGET', 'residus', data=df, linestyle='', marker='o', markersize=2)\n",
    "#ax.set_ylim(ymin=0.1)\n",
    "ax.set_title('|ypred-y|/y vs y on valid ln Q - original split')\n",
    "plt.show()\n",
    "\n",
    "ax = plt.subplot()\n",
    "ax.set_xscale(\"log\", nonposx='clip')\n",
    "ax.set_yscale(\"log\", nonposy='clip')\n",
    "plt.plot( 'yt', 'residus', data=dftrain, linestyle='', marker='o', markersize=2, alpha=0.6)\n",
    "ax.set_title('|ypred-y|/y vs y on train ln Q - original split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### target impact des NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = fdf.copy()\n",
    "tdf['TARGET'] = output[\"TARGET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tdf[\"TARGET\"])\n",
    "plt.title(\"Y\")\n",
    "plt.ylabel(\" Frequency\")\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[tdf.NA < 0.1].shape\n",
    "#tdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(tdf[\"TARGET\"])\n",
    "plt.title(\"Y\")\n",
    "plt.ylabel(\" Frequency\")\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(1, 19):\n",
    "    l =i/20\n",
    "    plt.hist(tdf[tdf.NA > l][\"TARGET\"])\n",
    "    plt.title(\"Y >\"+ str(l))\n",
    "    plt.ylabel(\" Frequency\")\n",
    "    plt.yscale('log', nonposy='clip')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = sns.scatterplot(x=tdf[\"NA\"], y=tdf[\"TARGET\"], data=tdf)\n",
    "ax.set_xlim(0,1)\n",
    "ax.set_ylim(0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tdf[tdf['volatility 09:30:00'].isna()]\n",
    "\n",
    "\n",
    "plt.hist(tdf[tdf['volatility 09:30:00'].isna()][\"TARGET\"])\n",
    "plt.title(\"target for NA at 9:30\")\n",
    "plt.ylabel(\" Frequency\")\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### investigating depth (all step 1 features nbut dates)\n",
    "- pareil: indices d'overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Lm = []\n",
    "Lt= []\n",
    "Ld = []\n",
    "r = range(3, 7)\n",
    "for i in r:    \n",
    "    xbr= xgb.XGBRegressor(max_depth=i, learning_rate=0.1, n_estimators=140,  subsample=1,  \n",
    "                       random_state=42, n_jobs=-1)\n",
    "    xbr.fit(X, logy)\n",
    "\n",
    "    logy_pred = xbr.predict(X_valid)\n",
    "    y_pred = np.exp(logy_pred )\n",
    "    m= MAPE(y_true, y_pred)\n",
    "    logyt = xbr.predict(X)\n",
    "    yt = np.exp(logyt)\n",
    "    t= MAPE(y, yt)                \n",
    "    print(\"mape valid:    \"+ str(i), m)\n",
    "    print(\"mape training: \"+ str(i), t)\n",
    "    print(\"delta\", m-t)\n",
    "    print(\"--------------\")              \n",
    "    \n",
    "    Lm.append(m)\n",
    "    Lt.append(t)\n",
    "    Ld.append(m-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r, Lt, label = \"train\")\n",
    "plt.plot(r, Lm, label = \"valid\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding mean, std,... for valatilities as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp= fdf.loc[:, 'volatility 09:30:00':'volatility 13:55:00']\n",
    "\n",
    "temp.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = fdf.loc[:, 'volatility 09:30:00':].columns\n",
    "temp\n",
    "fdf.groupby(['product_id'])[temp].mean().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add \n",
    "\n",
    "def build_features_2(df):\n",
    "\n",
    "    result= df.copy()\n",
    "\n",
    "    temp= df.loc[:, 'volatility 09:30:00':'volatility 13:55:00']\n",
    "    #result[\"volmean\"] = temp.mean(axis=1)\n",
    "    \n",
    "    result[\"volmean\"] = temp.mean(axis=1)\n",
    "    result[\"volstd\"] = temp.std(axis=1)\n",
    "    result[\"volmin\"] = temp.min(axis=1)\n",
    "    result[\"volmax\"] = temp.max(axis=1)\n",
    "    result[\"vol25%\"] = temp.quantile(0.25, axis=1)\n",
    "    result[\"vol50%\"] = temp.quantile(0.50, axis=1)\n",
    "    result[\"vol75%\"] = temp.quantile(0.75, axis=1)\n",
    "    #result[\"vol95%\"] = temp.quantile(0.95, axis=1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fdf2 = build_features_2(fdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test2 = build_features_2(build_features_1(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp = fdf2.loc[:, \"NA\":]\n",
    "for i in tmp.columns:\n",
    "    print(i)\n",
    "    plt.hist(fdf2[i])\n",
    "    plt.title(i)\n",
    "    plt.ylabel(\" Frequency\")\n",
    "    plt.yscale('log', nonposy='clip')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.concat([fdf2, output[\"TARGET\"]], axis=1)\n",
    "valid = df[df.date>=1563]\n",
    "training = df[df.date<1563]\n",
    "X = training.iloc[:,1:-1]\n",
    "y = training.iloc[:,-1]\n",
    "\n",
    "X_valid = valid.iloc[:,1:-1]\n",
    "y_true = valid.iloc[:,-1]\n",
    "logy = np.log(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Lm = []\n",
    "Lt= []\n",
    "Ld = []\n",
    "r = range(80, 170, 30)\n",
    "for i in r:    \n",
    "    xbr= xgb.XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=i,  subsample=1,  \n",
    "                       random_state=42, n_jobs=-1)\n",
    "    xbr.fit(X, logy)\n",
    "\n",
    "    logy_pred = xbr.predict(X_valid)\n",
    "    y_pred = np.exp(logy_pred )\n",
    "    m= MAPE(y_true, y_pred)\n",
    "    logyt = xbr.predict(X)\n",
    "    yt = np.exp(logyt)\n",
    "    t= MAPE(y, yt)                \n",
    "    print(\"mape valid:    \"+ str(i), m)\n",
    "    print(\"mape training: \"+ str(i), t)\n",
    "    print(\"delta\", m-t)\n",
    "    print(\"--------------\")              \n",
    "    \n",
    "    Lm.append(m)\n",
    "    Lt.append(t)\n",
    "    Ld.append(m-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = xbr.feature_importances_\n",
    "feature_importances = pd.DataFrame(importances, index = X.columns, \n",
    "                                   columns=['importance']).sort_values('importance', ascending=False)\n",
    "plt.bar(feature_importances.index, feature_importances[\"importance\"])\n",
    "plt.show()\n",
    "\n",
    "feature_importances.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "post = X_valid.copy()\n",
    "post[\"y_true\"] = y_true.copy()\n",
    "post[\"y_pred\"] = y_pred.copy()\n",
    "post[\"residus\"] = MAPE_residus(post[\"y_true\"], post[\"y_pred\"])\n",
    "\n",
    "\n",
    "#permodel.sort_values(by=\"MAPE\", ascending=False)[permodel.MAPE > 30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m  == MAPE(post[\"y_true\"], post[\"y_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post[post.product_id==211].sort_values(by=\"residus\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lmp = []\n",
    "Lna = []\n",
    "Lnat = []\n",
    "r = range(1, len(post.product_id.unique()) +1)\n",
    "for i in r:\n",
    "    mape_ = MAPE(post[post.product_id==i].y_true, post[post.product_id==i].y_pred)\n",
    "    Lmp.append(mape_)\n",
    "    Lna.append(post[post.product_id==i].NA.mean())\n",
    "    Lnat.append(test2[test2.product_id==i].NA.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(r), len(Lmp), len(Lna))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d={'product_id':r, 'MAPE':Lmp, \"NA\":Lna,\"NA test\": Lnat}\n",
    "perproduct = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "perproduct.sort_values(by=\"MAPE\", ascending=False)[perproduct.MAPE>30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perproduct.sort_values(by=\"MAPE\", ascending=False)[perproduct.NA>0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perproduct.sort_values(by=\"MAPE\", ascending=False)[perproduct[\"NA test\"]>0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(r, Lmp, label = \"MAPE\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"MAPE per product\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(r, Lna, label = \"NA\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"NA per product\")\n",
    "plt.show()\n",
    "\n",
    "ax = plt.subplot()\n",
    "ax.set_xscale(\"log\", nonposx='clip')\n",
    "#ax.set_yscale(\"log\", nonposy='clip')\n",
    "plt.plot( 'NA', 'MAPE', data=perproduct, linestyle='', marker='o', markersize=2)\n",
    "#ax.set_ylim(ymin=0.1)\n",
    "ax.set_title('MAPE vs NA per product')\n",
    "plt.show()\n",
    "\n",
    "ax = plt.subplot()\n",
    "ax.set_xscale(\"log\", nonposx='clip')\n",
    "#ax.set_yscale(\"log\", nonposy='clip')\n",
    "plt.plot( 'NA', 'MAPE', data=perproduct, linestyle='', marker='o', markersize=2)\n",
    "ax.set_ylim(ymax=40)\n",
    "ax.set_title('MAPE vs NA per product. Cliped for MAPE>40')\n",
    "plt.show()\n",
    "\n",
    "ax = plt.subplot()\n",
    "#ax.set_xscale(\"log\", nonposx='clip')\n",
    "#ax.set_yscale(\"log\", nonposy='clip')\n",
    "plt.plot( 'NA', 'NA test', data=perproduct, linestyle='', marker='o', markersize=2)\n",
    "#ax.set_ylim(ymax=40)\n",
    "ax.set_title('NA test vs NA per product. ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 211\n",
    "print(\"MAPE product_id=\" +str(i), MAPE(post[post.product_id==i].y_true, post[post.product_id==i].y_pred))\n",
    "print(\"NA mean product_id=\" +str(i), post[post.product_id==i].NA.mean())\n",
    "\n",
    "i = 31\n",
    "print(\"MAPE product_id=\" +str(i), MAPE(post[post.product_id==i].y_true, post[post.product_id==i].y_pred))\n",
    "print(\"NA mean product_id=\" +str(i), post[post.product_id==i].NA.mean())\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### investigating depth when step 2 features (volmean, volstd, ...) are added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Lm = []\n",
    "Lt= []\n",
    "Ld = []\n",
    "r = range(3, 7)\n",
    "for i in r:    \n",
    "    xbr= xgb.XGBRegressor(max_depth=i, learning_rate=0.1, n_estimators=140,  subsample=1,  \n",
    "                       random_state=42, n_jobs=-1)\n",
    "    xbr.fit(X, logy)\n",
    "\n",
    "    logy_pred = xbr.predict(X_valid)\n",
    "    y_pred = np.exp(logy_pred )\n",
    "    m= MAPE(y_true, y_pred)\n",
    "    logyt = xbr.predict(X)\n",
    "    yt = np.exp(logyt)\n",
    "    t= MAPE(y, yt)                \n",
    "    print(\"mape valid:    \"+ str(i), m)\n",
    "    print(\"mape training: \"+ str(i), t)\n",
    "    print(\"delta\", m-t)\n",
    "    print(\"--------------\")              \n",
    "    \n",
    "    Lm.append(m)\n",
    "    Lt.append(t)\n",
    "    Ld.append(m-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r, Lt, label = \"train\")\n",
    "plt.plot(r, Lm, label = \"valid\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2[test2.product_id==211].NA.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### post proces ypred: when NA too big, replace per 0\n",
    "meme en ce cantonant Ã  211 c'est marginale\n",
    "en remplacant par la moyenne c'est pire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post.sort_values(by=\"residus\", ascending=True)[post.NA>0.7].residus.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post[\"y_pred2\"] = post.y_pred.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post.y_pred2[post.NA>0.7] =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" MAPE without rewrite\", MAPE(post.y_true, post.y_pred))\n",
    "print(\" MAPE with rewrite\", MAPE(post.y_true, post.y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perproduct.sort_values(by=\"MAPE\", ascending=False)[perproduct.NA>0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post[post.NA>0.8][post.product_id==211].loc[:, \"NA\":].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t =0.7\n",
    "y_pred211mean = post.y_pred[post.NA>t][post.product_id==211].mean()\n",
    "\n",
    "post[\"y_pred3\"] = post.y_pred.copy()\n",
    "post[\"y_pred4\"] = post.y_pred.copy()\n",
    "\n",
    "for i in post.y_pred3[post.NA>t][post.product_id==211].index:\n",
    "    post.loc[i, \"y_pred3\"]=0\n",
    "    post.loc[i, \"y_pred4\"]=y_pred211mean\n",
    "print(\" MAPE without rewrite\", MAPE(post.y_true, post.y_pred))\n",
    "print(\" MAPE with rewrite 1\", MAPE(post.y_true, post.y_pred2))\n",
    "print(\" MAPE with rewrite 2\", MAPE(post.y_true, post.y_pred3))\n",
    "i = 211\n",
    "print(\"without rewrite MAPE product_id=\" +str(i), \n",
    "      MAPE(post[post.product_id==i].y_true, post[post.product_id==i].y_pred))\n",
    "print(\"rewrite 1 MAPE product_id=\" +str(i), \n",
    "      MAPE(post[post.product_id==i].y_true, post[post.product_id==i].y_pred2))\n",
    "print(\"rewrite 2 MAPE product_id=\" +str(i), \n",
    "      MAPE(post[post.product_id==i].y_true, post[post.product_id==i].y_pred3))\n",
    "print(\"rewrite 3 MAPE product_id=\" +str(i), \n",
    "      MAPE(post[post.product_id==i].y_true, post[post.product_id==i].y_pred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post[post.NA>t][post.product_id==211].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in [0.5, 0.6, 0.7, 0.9, 0.95]:\n",
    "    post[\"y_pred3\"] = post.y_pred.copy()\n",
    "    for i in post.y_pred3[post.NA>t][post.product_id==211].index:\n",
    "        post.loc[i, \"y_pred3\"]=0\n",
    "\n",
    "    i = 211\n",
    "\n",
    "    print(\"rewrite 2 MAPE product_id=\" +str(i), t, \n",
    "          MAPE(post[post.product_id==i].y_true, post[post.product_id==i].y_pred3))\n",
    "    \n",
    "print(\"without rewrite MAPE product_id=\" +str(i), \n",
    "      MAPE(post[post.product_id==i].y_true, post[post.product_id==i].y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training without 211\n",
    "\n",
    "\n",
    "reference\n",
    "mape valid:    140 24.12963942019092\n",
    "mape training: 140 24.36233000079195\n",
    "delta -0.2326905806010302\n",
    "\n",
    "result: le training s'ameliore (evidement), mais valid se degrade legerement\n",
    "mape valid:    140 24.14341279902005\n",
    "mape training: 140 23.872494337420903\n",
    "delta 0.27091846159914823"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.concat([fdf2, output[\"TARGET\"]], axis=1)\n",
    "valid = df[df.date>=1563]\n",
    "training = df[df.date<1563][df.product_id!=211]\n",
    "X = training.iloc[:,1:-1]\n",
    "y = training.iloc[:,-1]\n",
    "\n",
    "X_valid = valid.iloc[:,1:-1]\n",
    "y_true = valid.iloc[:,-1]\n",
    "logy = np.log(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Lm = []\n",
    "Lt= []\n",
    "Ld = []\n",
    "r = range(80, 170, 30)\n",
    "for i in r:    \n",
    "    xbr= xgb.XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=i,  subsample=1,  \n",
    "                       random_state=42, n_jobs=-1)\n",
    "    xbr.fit(X, logy)\n",
    "\n",
    "    logy_pred = xbr.predict(X_valid)\n",
    "    y_pred = np.exp(logy_pred )\n",
    "    m= MAPE(y_true, y_pred)\n",
    "    logyt = xbr.predict(X)\n",
    "    yt = np.exp(logyt)\n",
    "    t= MAPE(y, yt)                \n",
    "    print(\"mape valid:    \"+ str(i), m)\n",
    "    print(\"mape training: \"+ str(i), t)\n",
    "    print(\"delta\", m-t)\n",
    "    print(\"--------------\")              \n",
    "    \n",
    "    Lm.append(m)\n",
    "    Lt.append(t)\n",
    "    Ld.append(m-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### droping less important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf2b = fdf2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances[feature_importances.importance<0.01].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fdf2b.drop(feature_importances[feature_importances.importance<0.01].index, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.concat([fdf2b, output[\"TARGET\"]], axis=1)\n",
    "valid = df[df.date>=1563]\n",
    "training = df[df.date<1563]\n",
    "X = training.iloc[:,1:-1]\n",
    "y = training.iloc[:,-1]\n",
    "\n",
    "X_valid = valid.iloc[:,1:-1]\n",
    "y_true = valid.iloc[:,-1]\n",
    "logy = np.log(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Lm = []\n",
    "Lt= []\n",
    "Ld = []\n",
    "r = range(80, 170, 30)\n",
    "for i in r:    \n",
    "    xbr= xgb.XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=i,  subsample=1,  \n",
    "                       random_state=42, n_jobs=-1)\n",
    "    xbr.fit(X, logy)\n",
    "\n",
    "    logy_pred = xbr.predict(X_valid)\n",
    "    y_pred = np.exp(logy_pred )\n",
    "    m= MAPE(y_true, y_pred)\n",
    "    logyt = xbr.predict(X)\n",
    "    yt = np.exp(logyt)\n",
    "    t= MAPE(y, yt)                \n",
    "    print(\"mape valid:    \"+ str(i), m)\n",
    "    print(\"mape training: \"+ str(i), t)\n",
    "    print(\"delta\", m-t)\n",
    "    print(\"--------------\")              \n",
    "    \n",
    "    Lm.append(m)\n",
    "    Lt.append(t)\n",
    "    Ld.append(m-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importances = xbr.feature_importances_\n",
    "feature_importances = pd.DataFrame(importances, index = X.columns, \n",
    "                                   columns=['importance']).sort_values('importance', ascending=False)\n",
    "plt.bar(feature_importances.index, feature_importances[\"importance\"])\n",
    "plt.show()\n",
    "\n",
    "feature_importances.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = fdf2.iloc[:, 2:].columns\n",
    "temp\n",
    "fdf2.groupby(['product_id'])[temp].mean().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trying a separate model per product\n",
    "\n",
    "ref \n",
    "mape valid:    110 24.204416648914243\n",
    "mape training: 110 24.499145575072742\n",
    "delta -0.29472892615849844"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training.product_id.unique()\n",
    "#training[training.product_id==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.concat([fdf2, output[\"TARGET\"]], axis=1)\n",
    "valid = df[df.date>=1563]\n",
    "training = df[df.date<1563]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "Lm = []\n",
    "Lt= []\n",
    "Ld = []\n",
    "\n",
    "for i in range(1, 319):\n",
    "    X = training[training.product_id==i].iloc[:,1:-1]\n",
    "    y = training[training.product_id==i].iloc[:,-1]\n",
    "\n",
    "    X_valid = valid[valid.product_id==i].iloc[:,1:-1]\n",
    "    y_true = valid[valid.product_id==i].iloc[:,-1]\n",
    "    logy = np.log(y)\n",
    "    xbr= xgb.XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=50,  subsample=1,  #110\n",
    "                       random_state=42, n_jobs=-1)\n",
    "    xbr.fit(X, logy)\n",
    "\n",
    "    logy_pred = xbr.predict(X_valid)\n",
    "    y_pred = np.exp(logy_pred )\n",
    "    m= MAPE(y_true, y_pred)\n",
    "    logyt = xbr.predict(X)\n",
    "    yt = np.exp(logyt)\n",
    "    t= MAPE(y, yt)                \n",
    "    print(\"mape valid for product_id:    \"+ str(i), m)\n",
    "    print(\"mape training for product_id: \"+ str(i), t)\n",
    "    print(\"delta\", m-t)\n",
    "    print(\"--------------\") \n",
    "    \n",
    "    if (i==211):\n",
    "        X_valid211 = X_valid.copy()\n",
    "        y_pred211 = y_pred.copy()\n",
    "        y_true211 = y_true.copy()\n",
    "        print(\"saving product_id\")\n",
    "    \n",
    "    Lm.append(m)\n",
    "    Lt.append(t)\n",
    "    Ld.append(m-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = range(1, 319)\n",
    "plt.plot(r, Lt, label = \"train\")\n",
    "plt.plot(r, Lm, label = \"valid\")\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df211.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d={'product_id':r, 'MAPE':Lm}\n",
    "permodel = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permodel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permodel.sort_values(by=\"MAPE\", ascending=False)[permodel.MAPE > 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r= range(1, len(permodel)+1)\n",
    "plt.plot(r, permodel.sort_values(by=\"MAPE\", ascending=False).MAPE, label = \"MAPE\")\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"MAPE per product\")\n",
    "plt.show()\n",
    "\n",
    "s = permodel.sort_values(by=\"MAPE\", ascending=False)[permodel.MAPE < 30].MAPE\n",
    "r= range(1, len(s)+1)\n",
    "plt.plot(r,s , label = \"MAPE\")\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"MAPE per product\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product10 = list(permodel.sort_values(by=\"MAPE\", ascending=False).head(10).product_id)\n",
    "df[df.product_id.isin(product10)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.product_id==211].loc[:, :'NA'].head(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df211= df[df.product_id==211]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df211[\"NA\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( 'TARGET', 'NA', data=df211, linestyle='', marker='o', markersize=2)\n",
    "#plt.yscale('log', nonposy='clip')\n",
    "#ax.set_xlim(0,)\n",
    "#ax.set_ylim(-0.1,)\n",
    "plt.title(\"NA vs y product 211\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot( 'TARGET', 'NA', data=df[df.NA>0.2], linestyle='', marker='o', markersize=2)\n",
    "#plt.yscale('log', nonposy='clip')\n",
    "#ax.set_xlim(0,)\n",
    "#ax.set_ylim(-0.1,)\n",
    "plt.title(\"NA vs y for NA>0.2\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "ax = plt.subplot()\n",
    "ax.set_xscale(\"log\", nonposx='clip')\n",
    "#ax.set_yscale(\"log\", nonposy='clip')\n",
    "plt.plot( 'TARGET', 'NA', data=df211, linestyle='', marker='o', markersize=2, alpha=0.6)\n",
    "#ax.set_ylim(ymin=0.1)\n",
    "ax.set_title('NA vs y product 211')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( 'TARGET', 'NA', data=df[df.NA>0.2], linestyle='', marker='o', markersize=2)\n",
    "#plt.yscale('log', nonposy='clip')\n",
    "#ax.set_xlim(0,)\n",
    "#ax.set_ylim(-0.1,)\n",
    "plt.title(\"NA vs y for NA>0.2\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "ax = plt.subplot()\n",
    "ax.set_xscale(\"log\", nonposx='clip')\n",
    "#ax.set_yscale(\"log\", nonposy='clip')\n",
    "plt.plot( 'TARGET', 'NA', data=df[df.NA>0.2], linestyle='', marker='o', markersize=2, alpha=0.6)\n",
    "#ax.set_ylim(ymin=0.1)\n",
    "ax.set_title('NA vs y for NA>0.2 ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid211 = X_valid211.copy()\n",
    "valid211['y_pred']= y_pred211.copy()\n",
    "valid211['y_true']= y_true211.copy()\n",
    "valid211['residus'] = MAPE_residus(y_true211, y_pred211)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_product(X_valid211, y_pred211, y_true211, rmax=None):\n",
    "    valid211 = X_valid211.copy()\n",
    "    valid211['y_pred']= y_pred211.copy()\n",
    "    valid211['y_true']= y_true211.copy()\n",
    "    valid211['residus'] = MAPE_residus(y_true211, y_pred211)\n",
    "    ax = plt.subplot()\n",
    "    #ax.set_xscale(\"log\", nonposx='clip')\n",
    "    #ax.set_yscale(\"log\", nonposy='clip')\n",
    "    plt.plot( 'NA', 'residus', data=valid211, linestyle='', marker='o', markersize=2)\n",
    "    ax.set_ylim(ymax=rmax)\n",
    "    plt.title(\"residus vs NA for product 211 - one outlier out of scale\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_product(X_valid211, y_pred211, y_true211, rmax=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une partie desresidus semblemnt s'envoler pour NA> 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features engineering: adding the means by products\n",
    "product_id: 1..318"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = fdf2.loc[fdf2.product_id==31, \"volatility 09:30:00\":\"volatility 13:55:00\"].shape[0]\n",
    "c1 = fdf2.loc[fdf2.product_id==31, \"volatility 09:30:00\":\"volatility 13:55:00\"].shape[1]\n",
    "\n",
    "np.nanmean(fdf2.loc[fdf2.product_id==31, \"volatility 09:30:00\":\"volatility 13:55:00\"].values.reshape((r1*c1, 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def build_features_3(fdf2):\n",
    "    col = fdf2.columns[2:]\n",
    "    fdf3= fdf2.copy()\n",
    "    for p in range(1, 319):\n",
    "        v = fdf2.loc[fdf2.product_id==p, \"volatility 09:30:00\":\"volatility 13:55:00\"]\n",
    "        r1 = v.shape[0]\n",
    "        c1 = v.shape[1]\n",
    "        fdf3.loc[fdf3.product_id==p, \"product_mean\"] = np.nanmean(fdf2.loc[fdf2.product_id==p, \"volatility 09:30:00\":\"volatility 13:55:00\"].values.reshape((r1*c1, 1)))\n",
    "        for i in col:\n",
    "            #col = \"return_sum\"\n",
    "            temp = fdf2.loc[fdf2.product_id==p, i].mean()\n",
    "            #print(i, temp)\n",
    "            fdf3.loc[fdf3.product_id==p, i] = fdf3.loc[fdf3.product_id==p, i] - temp\n",
    "    return fdf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fdf3 = build_features_3(fdf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf3[fdf3.product_id==4].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf2[fdf2.product_id==4].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# as is does not work\n",
    "product mean has high importance but except this, performance are worst (and without it awfull)\n",
    "\n",
    "avec tout\n",
    "mape valid:    140 24.2883711902457\n",
    "mape training: 140 24.52647198244592\n",
    "delta -0.23810079220021763\n",
    "\n",
    "importance\n",
    "product_mean \t0.208163\n",
    "volmean \t0.134694\n",
    "return_zeros \t0.080612\n",
    "vol75% \t0.048980\n",
    "volatility 13:55:00 \t0.043878\n",
    "NA \t0.034694\n",
    "volatility 13:50:00 \t0.033673\n",
    "\n",
    "sans product_mean\n",
    "mape valid:    140 26.030174610130068\n",
    "mape training: 140 26.279038534587595\n",
    "delta -0.24886392445752747\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.concat([fdf3, output[\"TARGET\"]], axis=1)\n",
    "valid = df[df.date>=1563]\n",
    "training = df[df.date<1563]\n",
    "X = training.iloc[:,2:-1]\n",
    "y = training.iloc[:,-1]\n",
    "\n",
    "X_valid = valid.iloc[:,2:-1]\n",
    "y_true = valid.iloc[:,-1]\n",
    "logy = np.log(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "Lm = []\n",
    "Lt= []\n",
    "Ld = []\n",
    "r = range(80, 170, 30)\n",
    "for i in r:    \n",
    "    xbr= xgb.XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=i,  subsample=1,  \n",
    "                       random_state=42, n_jobs=-1)\n",
    "    xbr.fit(X, logy)\n",
    "\n",
    "    logy_pred = xbr.predict(X_valid)\n",
    "    y_pred = np.exp(logy_pred )\n",
    "    m= MAPE(y_true, y_pred)\n",
    "    logyt = xbr.predict(X)\n",
    "    yt = np.exp(logyt)\n",
    "    t= MAPE(y, yt)                \n",
    "    print(\"mape valid:    \"+ str(i), m)\n",
    "    print(\"mape training: \"+ str(i), t)\n",
    "    print(\"delta\", m-t)\n",
    "    print(\"--------------\")              \n",
    "    \n",
    "    Lm.append(m)\n",
    "    Lt.append(t)\n",
    "    Ld.append(m-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = xbr.feature_importances_\n",
    "feature_importances = pd.DataFrame(importances, index = X.columns, \n",
    "                                   columns=['importance']).sort_values('importance', ascending=False)\n",
    "plt.bar(feature_importances.index, feature_importances[\"importance\"])\n",
    "plt.show()\n",
    "\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trying again without substracting mean to each features, just adding product_mean as a start\n",
    "\n",
    "reference\n",
    "mape valid:    140 24.12963942019092\n",
    "mape training: 140 24.36233000079195\n",
    "delta -0.2326905806010302\n",
    "\n",
    "avec product_mean (sans product id)\n",
    "mape valid:    140 24.04295602135195\n",
    "mape training: 140 24.257907302708894\n",
    "delta -0.2149512813569423\n",
    "\n",
    "et product_std en plus: ne change pas grand chose\n",
    "mape valid:    140 24.04486471918276\n",
    "mape training: 140 24.27193480127085\n",
    "delta -0.22707008208809043\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def build_features_4(fdf2):\n",
    "    col = fdf2.columns[2:]\n",
    "    fdf4= fdf2.copy()\n",
    "    for p in range(1, 319):\n",
    "        v = fdf2.loc[fdf2.product_id==p, \"volatility 09:30:00\":\"volatility 13:55:00\"]\n",
    "        r1 = v.shape[0]\n",
    "        c1 = v.shape[1]\n",
    "        fdf4.loc[fdf3.product_id==p, \"product_mean\"] = np.nanmean(fdf2.loc[fdf2.product_id==p, \"volatility 09:30:00\":\"volatility 13:55:00\"].values.reshape((r1*c1, 1)))\n",
    "        fdf4.loc[fdf3.product_id==p, \"product_std\"] = np.nanstd(fdf2.loc[fdf2.product_id==p, \"volatility 09:30:00\":\"volatility 13:55:00\"].values.reshape((r1*c1, 1)))\n",
    "    return fdf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fdf4 = build_features_4(fdf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test4 = build_features_4(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.concat([fdf4, output[\"TARGET\"]], axis=1)\n",
    "valid = df[df.date>=1563]\n",
    "training = df[df.date<1563]\n",
    "X = training.iloc[:,2:-1]\n",
    "y = training.iloc[:,-1]\n",
    "\n",
    "X_valid = valid.iloc[:,2:-1]\n",
    "y_true = valid.iloc[:,-1]\n",
    "logy = np.log(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "Lm = []\n",
    "Lt= []\n",
    "Ld = []\n",
    "r = range(80, 170, 30)\n",
    "for i in r:    \n",
    "    xbr= xgb.XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=i,  subsample=1,  \n",
    "                       random_state=42, n_jobs=-1)\n",
    "    xbr.fit(X, logy)\n",
    "\n",
    "    logy_pred = xbr.predict(X_valid)\n",
    "    y_pred = np.exp(logy_pred )\n",
    "    m= MAPE(y_true, y_pred)\n",
    "    logyt = xbr.predict(X)\n",
    "    yt = np.exp(logyt)\n",
    "    t= MAPE(y, yt)                \n",
    "    print(\"mape valid:    \"+ str(i), m)\n",
    "    print(\"mape training: \"+ str(i), t)\n",
    "    print(\"delta\", m-t)\n",
    "    print(\"--------------\")              \n",
    "    \n",
    "    Lm.append(m)\n",
    "    Lt.append(t)\n",
    "    Ld.append(m-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = xbr.feature_importances_\n",
    "feature_importances = pd.DataFrame(importances, index = X.columns, \n",
    "                                   columns=['importance']).sort_values('importance', ascending=False)\n",
    "plt.bar(feature_importances.index, feature_importances[\"importance\"])\n",
    "plt.show()\n",
    "\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Lm = []\n",
    "Lt= []\n",
    "Ld = []\n",
    "r = range(3, 7)\n",
    "for i in r:    \n",
    "    xbr= xgb.XGBRegressor(max_depth=i, learning_rate=0.1, n_estimators=140,  subsample=1,  \n",
    "                       random_state=42, n_jobs=-1)\n",
    "    xbr.fit(X, logy)\n",
    "\n",
    "    logy_pred = xbr.predict(X_valid)\n",
    "    y_pred = np.exp(logy_pred )\n",
    "    m= MAPE(y_true, y_pred)\n",
    "    logyt = xbr.predict(X)\n",
    "    yt = np.exp(logyt)\n",
    "    t= MAPE(y, yt)                \n",
    "    print(\"mape valid:    \"+ str(i), m)\n",
    "    print(\"mape training: \"+ str(i), t)\n",
    "    print(\"delta\", m-t)\n",
    "    print(\"--------------\")              \n",
    "    \n",
    "    Lm.append(m)\n",
    "    Lt.append(t)\n",
    "    Ld.append(m-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r, Lt, label = \"train\")\n",
    "plt.plot(r, Lm, label = \"valid\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = xbr.feature_importances_\n",
    "feature_importances = pd.DataFrame(importances, index = X.columns, \n",
    "                                   columns=['importance']).sort_values('importance', ascending=False)\n",
    "plt.bar(feature_importances.index, feature_importances[\"importance\"])\n",
    "plt.show()\n",
    "\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's push with depth = 4 (instead of the default 3)\n",
    "\n",
    "ref for depth 3 for 140 estimators\n",
    "mape valid:    3 24.12963942019092\n",
    "mape training: 3 24.36233000079195\n",
    "delta -0.2326905806010302"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Lm = []\n",
    "Lt= []\n",
    "Ld = []\n",
    "r = range(100, 210, 10)\n",
    "for i in r:    \n",
    "    xbr= xgb.XGBRegressor(max_depth=4, learning_rate=0.1, n_estimators=i,  subsample=1,  \n",
    "                       random_state=42, n_jobs=-1)\n",
    "    xbr.fit(X, logy)\n",
    "\n",
    "    logy_pred = xbr.predict(X_valid)\n",
    "    y_pred = np.exp(logy_pred )\n",
    "    m= MAPE(y_true, y_pred)\n",
    "    logyt = xbr.predict(X)\n",
    "    yt = np.exp(logyt)\n",
    "    t= MAPE(y, yt)                \n",
    "    print(\"mape valid:    \"+ str(i), m)\n",
    "    print(\"mape training: \"+ str(i), t)\n",
    "    print(\"delta\", m-t)\n",
    "    print(\"--------------\")              \n",
    "    \n",
    "    Lm.append(m)\n",
    "    Lt.append(t)\n",
    "    Ld.append(m-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r, Lt, label = \"train\")\n",
    "plt.plot(r, Lm, label = \"valid\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trying regulatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for l in [2, 3, 4, 5]:\n",
    "    Lm = []\n",
    "    Lt= []\n",
    "    Ld = []\n",
    "    r = range(130, 200, 10)\n",
    "    print(\"--------------------------------\")\n",
    "    print(\"lambda\", l)\n",
    "    for i in r:    \n",
    "        xbr= xgb.XGBRegressor(max_depth=4, learning_rate=0.1, n_estimators=i,  subsample=1,  \n",
    "                           random_state=42, n_jobs=-1, reg_lambda=l)\n",
    "        xbr.fit(X, logy)\n",
    "\n",
    "        logy_pred = xbr.predict(X_valid)\n",
    "        y_pred = np.exp(logy_pred )\n",
    "        m= MAPE(y_true, y_pred)\n",
    "        logyt = xbr.predict(X)\n",
    "        yt = np.exp(logyt)\n",
    "        t= MAPE(y, yt)                \n",
    "        print(\"mape valid:    \"+ str(i), m)\n",
    "        print(\"mape training: \"+ str(i), t)\n",
    "        print(\"delta\", m-t)\n",
    "        print(\"--------------\")              \n",
    "\n",
    "        Lm.append(m)\n",
    "        Lt.append(t)\n",
    "        Ld.append(m-t)\n",
    "\n",
    "    plt.plot(r, Lt, label = \"train\")\n",
    "    plt.plot(r, Lm, label = \"valid\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(\"train/valid for estimators\" +str(i) + \" and lambda\" +  str(l))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = fdf4.iloc[:,2:]\n",
    "y = output[\"TARGET\"]\n",
    "\n",
    "logy = np.log(y)\n",
    "X_test = test4.iloc[:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns == X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbr= xgb.XGBRegressor(max_depth=4, learning_rate=0.1, n_estimators=170,  subsample=1,  \n",
    "                           random_state=42, n_jobs=-1, reg_lambda=4)\n",
    "\n",
    "xbr.fit(X, logy)\n",
    "logy_pred = xbr.predict(X_test)\n",
    "y_pred = np.exp(logy_pred )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logyt = xbr.predict(X)\n",
    "yt = np.exp(logyt)\n",
    "\n",
    "print(MAPE(y, yt))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score challenge 24.37\n",
    "#mape valid:    170 23.971816721617984\n",
    "#mape training: 170 23.99550792956179\n",
    "\n",
    "d = {'TARGET':y_pred}\n",
    "result = pd.DataFrame(data=d)\n",
    "result.to_csv('C:\\\\Users\\\\i053131\\\\Desktop\\\\Epilepsie\\\\CFM\\\\data\\\\processed\\\\xgb_feature_4_d4_e170_l4.csv', \n",
    "              sep = \";\",index_label=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variability of return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
